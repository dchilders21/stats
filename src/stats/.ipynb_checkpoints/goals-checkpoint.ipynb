{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIDE NOTE: would love to get any critiques, hints, tips, oberservations on any level not just on ML (but obviously ML as the priority).  Beginner Python learner (though I took the Full Stack Course) so any suggestions there are welcome.  Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to import these libs plus setting some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import renders as rs\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.cm as cm\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "# Might need to change the path of the included libraries.\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/match_stats.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_model.py')\n",
    "sys.path.append('/anaconda/envs/stats/lib/python3.5/site-packages')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/model_libs.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_data.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats')\n",
    "# print(sys.path)\n",
    "os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "#print(os.getcwd())\n",
    "\n",
    "from stats import form_data, match_stats, model_libs, form_model, predict_matches\n",
    "\n",
    "# Variables\n",
    "round_number = 27 # for MLS only\n",
    "target_col = 'points'\n",
    "ignore_cols = ['match_id', 'team_id', 'team_name', 'opp_id', 'opp_name', 'scheduled', 'games_played', 'round']\n",
    "sub_cols = ['current_formation', 'avg_goals_against', 'goal_diff', 'win_percentage', 'sos',\n",
    "           'opp_win_percentage', 'opp_sos', 'current_team_yellow_cards', 'current_team_corner_kicks', 'current_team_first_half_goals', 'current_team_sec_half_goals', \n",
    "           'opp_team_yellow_cards', 'opp_team_corner_kicks', 'opp_team_first_half_goals', 'opp_team_sec_half_goals']\n",
    "\n",
    "all_models = ['log', 'svc', 'gmm', 'knn', 'gnb']\n",
    "\n",
    "\"\"\" this variable 'testing' should be false if using CSV's and not pulling from the database. \"\"\"\n",
    "testing = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling in the data either from the Database or the CSV (CSV for testers).  Data has some added features in between the database and 'raw_data'.  Essentially trying 3 different versions of the data in this set.  One version contains all the features.  Another version has the standard features plus the calculated features of the home/away stats for a matchup.  The last version has the standard features plus the 'extended features' which not only includes the stats of the two teams in the match up but also the current teams previous opponents and the current teams opponents of the team opponents in that matchup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_csv = 'round' + str(round_number) + '.csv'\n",
    "\n",
    "if testing:\n",
    "    raw_data = form_data.run_data()\n",
    "    raw_data.to_csv(data_csv)\n",
    "    print(\"Raw Data Saved to CSV\")\n",
    "else:\n",
    "    #Reading in a CSV adds the first index column\n",
    "    raw_data = pd.read_csv(data_csv)\n",
    "    raw_data = raw_data.drop(raw_data.columns[[0]], axis=1)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)\n",
    "\n",
    "print('Data Loaded...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Functions that trains basic models (and saves them if need be).  'load_models' will  load exisiting models to save time in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes Columns to Ignore and Splits the Target Column\n",
    "def split_target(data):\n",
    "    td = model_libs._clone_and_drop(data, ignore_cols)\n",
    "    (y, X) = model_libs._extract_target(td, target_col)\n",
    "    return X, y\n",
    "\n",
    "\"\"\"No Cross-Validation and No Tuning\"\"\"\n",
    "def train_models(round_num, X, y, models):\n",
    "    \n",
    "    if os.path.isdir(\"/models/\" + str(round_num)):\n",
    "        print('Making New Directory for the Round')\n",
    "        os.chdir('/Users/senzari/Machine_Learning/stats/src/models')\n",
    "        os.makedirs(str(round_num))\n",
    "        os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "        \n",
    "    finished_models = []\n",
    "    \n",
    "    for i in models:\n",
    "        \n",
    "        model_round = 'models/' + str(round_num) + '/' + str(i) + '_round_' + str(round_num) + '.pk1'\n",
    "        \n",
    "        \n",
    "        if i == 'log':\n",
    "            log = form_model.build_model(X, y, i)\n",
    "            joblib.dump(log, model_round)\n",
    "            finished_models.append(log)\n",
    "        elif i == 'svc':\n",
    "            svc = form_model.build_model(X, y, i)\n",
    "            joblib.dump(svc, model_round)\n",
    "            finished_models.append(svc)\n",
    "        elif i == 'gmm':\n",
    "            gmm = form_model.build_model(X, y, i)\n",
    "            joblib.dump(gmm, model_round)\n",
    "            finished_models.append(gmm)\n",
    "        elif i == 'knn':\n",
    "            kmeans = form_model.build_model(X, y, i)\n",
    "            joblib.dump(kmeans, model_round)\n",
    "            finished_models.append(kmeans)\n",
    "        elif i == 'gnb':\n",
    "            gnb = form_model.build_model(X, y, i)\n",
    "            joblib.dump(gnb, model_round)\n",
    "            finished_models.append(gnb)\n",
    "    \n",
    "    return finished_models\n",
    "        \n",
    "        \n",
    "def load_models(round_num):\n",
    "    \n",
    "    loaded_models = []\n",
    "    \n",
    "    for i in all_models:\n",
    "        model_round = 'models/' + str(round_num) + '/' + str(i) + '_round_' + str(round_num) + '.pk1'\n",
    "        if i == 'log':\n",
    "             log = joblib.load(model_round)\n",
    "             loaded_models.append(log)\n",
    "        if i == 'svc':\n",
    "             svc= joblib.load(model_round)\n",
    "             loaded_models.append(svc)\n",
    "        elif i == 'gmm':\n",
    "            gmm = joblib.load(model_round)\n",
    "            loaded_models.append(gmm)\n",
    "        elif i == 'knn':\n",
    "            kmeans = joblib.load(model_round)\n",
    "            loaded_models.append(kmeans)\n",
    "        elif i == 'gnb':\n",
    "            gnb = joblib.load(model_round)\n",
    "            loaded_models.append(gnb)\n",
    "        \n",
    "        print(\"Success :: Loaded - \" + str(i))\n",
    "        \n",
    "    return loaded_models\n",
    "\n",
    "# raw_X, raw_y = split_target(raw_data)\n",
    "\n",
    "# display(raw_data.head())\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\"\"\" Need to format data to convert goals scored to the correct category\"\"\"\n",
    "formatted_data = raw_data.drop('points', 1)\n",
    "\n",
    "def set_group(goals):\n",
    "    if goals >= 2:\n",
    "        return 1\n",
    "    elif goals < 2:\n",
    "        return 0\n",
    "    \n",
    "formatted_data['converted_goals'] = formatted_data.apply(lambda row: set_group(row['goals']), axis=1)\n",
    "\n",
    "#formatted_data = formatted_data.drop(sub_cols + ignore_cols + ['goals'], 1)\n",
    "\n",
    "def set_rpi_quartile(round_number, data, isCur):\n",
    "    power_rankings = pd.DataFrame()\n",
    "    power_list = []\n",
    "    \n",
    "    if isCur:\n",
    "        rpi = 'rpi'\n",
    "        team_id = 'team_id'\n",
    "    else:\n",
    "        rpi = 'opp_rpi'\n",
    "        team_id = 'opp_id'\n",
    "        \n",
    "    td = data.loc[(data[\"round\"] == round_number)]\n",
    "    \n",
    "    if not td.empty:\n",
    "        s = td.loc[:, [team_id, rpi]]\n",
    "        power_rankings = power_rankings.append(s, ignore_index=False)\n",
    "        power_rankings = power_rankings.sort_values(rpi, ascending=False)\n",
    "        for i, power in power_rankings.iterrows():\n",
    "            power_list.append(power)\n",
    "\n",
    "        pr = np.array(power_rankings.loc[:, rpi])\n",
    "        qqs = np.percentile(pr, [25, 50, 75, 100])\n",
    "        quartiles = [0, .3333, .6666, 1]\n",
    "        idx = len(pr)\n",
    "        for i in range(len(qqs)):\n",
    "            a = np.where( pr[0:idx] <= qqs[i] )\n",
    "            pr[a] = quartiles[i]\n",
    "            idx = a[0][0]\n",
    "            \n",
    "        return pr, power_rankings.index\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "mls_data = formatted_data.copy()\n",
    "mls_data = mls_data.loc[mls_data[\"team_id\"] < 41]\n",
    "mls_data['rpi_quartiled'] = pd.Series(None, index=mls_data.index) \n",
    "mls_min = mls_data['round'].min()\n",
    "mls_max = mls_data['round'].max() + 1\n",
    "\n",
    "epl_data = formatted_data.copy()\n",
    "epl_data = epl_data.loc[epl_data[\"team_id\"] > 40]\n",
    "epl_min = epl_data['round'].min()\n",
    "epl_max = epl_data['round'].max() + 1\n",
    "\n",
    "for x in range(mls_min, mls_max):\n",
    "    \n",
    "    power_rankings, idx = set_rpi_quartile(x, mls_data, True)\n",
    "    opp_power_rankings, opp_idx = set_rpi_quartile(x, mls_data, False)\n",
    "\n",
    "    if idx is not None:\n",
    "        mls_data.loc[idx, \"rpi_quartiled\"] = power_rankings\n",
    "        \n",
    "    if opp_idx is not None:\n",
    "        mls_data.loc[opp_idx, \"opp_rpi_quartiled\"] = opp_power_rankings\n",
    "        \n",
    "for x in range(epl_min, epl_max):\n",
    "    \n",
    "    power_rankings, idx = set_rpi_quartile(x, epl_data, True)\n",
    "    opp_power_rankings, opp_idx = set_rpi_quartile(x, epl_data, False)\n",
    "\n",
    "    if idx is not None:\n",
    "        epl_data.loc[idx, \"rpi_quartiled\"] = power_rankings\n",
    "        \n",
    "    if opp_idx is not None:\n",
    "        epl_data.loc[opp_idx, \"opp_rpi_quartiled\"] = opp_power_rankings\n",
    "        \n",
    "all_data = mls_data\n",
    "all_data = all_data.append(epl_data)\n",
    "display(all_data.head())\n",
    "regr_data = all_data.copy()\n",
    "# Drop the original RPI features and everything else\n",
    "all_data = all_data.drop(sub_cols + ignore_cols + ['goals'] + ['rpi', 'opp_rpi'], 1)\n",
    "display(all_data.head())\n",
    "#all_data.to_csv('all.csv')\n",
    "\n",
    "(all_y, all_X) = model_libs._extract_target(all_data, 'converted_goals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING CLASSIFICATION MODEL ON IF TEAMS SCORE 0-1 OR 1-2 ON GAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_features(data, drop_data, target, models):\n",
    "    \n",
    "    new_data = data.drop(drop_data, axis=1)\n",
    "    \n",
    "    display(new_data.head())\n",
    "    \n",
    "    (y, X) = model_libs._extract_target(new_data, target)\n",
    "    \n",
    "    models = train_models(round_number, X, y, models)\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "#### Running ALL Features \n",
    "models_test_1 = run_features(all_data, [], 'converted_goals', all_models)\n",
    "\n",
    "\"\"\"models_test_1 = run_features(formatted_data, [], 'converted_goals', ['log'])\n",
    "(f_y, f_X) = model_libs._extract_target(formatted_data, 'converted_goals')\n",
    "log = models_test_1[0]\n",
    "print(log.predict_proba(f_X))\n",
    "\n",
    "actual_y = pd.DataFrame(f_y.values, columns=['actual'])\n",
    "predictions = pd.DataFrame(log.predict(f_X), columns=['predictions'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "display(preds.head())\n",
    "print(accuracy)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since STANDARD and HOME and AWAY Features got the best score going to optimize and minimize off of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_RFECV(X, y):\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2),\n",
    "                  scoring='accuracy')\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    \n",
    "plot_RFECV(ha_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_RFE(X, y):\n",
    "    # Create the RFE object and rank each pixel\n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "    rfe.fit(X, y)\n",
    "    print(rfe.ranking_)\n",
    "    return rfe.ranking_\n",
    "\n",
    "(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals')\n",
    "\n",
    "# Taking all of Home/Away Data (even the Extended Features)\n",
    "rankings = create_RFE(ha_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Reordering the columns in the data to reflect the rankings \"\"\"\n",
    "\n",
    "rankings_name = []\n",
    "ranked_X = ha_X\n",
    "\n",
    "for r in range(len(rankings)):\n",
    "    column_name = ha_X.iloc[:, rankings[r]-1].name\n",
    "    rankings_name.append(column_name)\n",
    "\n",
    "ranked_X = ranked_X.reindex_axis(rankings_name, axis=1)\n",
    "display(ranked_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_ranked_models(X, y):\n",
    "    \n",
    "    for x in xrange(10, 20):\n",
    "        print('X :: {}'.format(x))\n",
    "        ranked_models = train_models(round_number, X.iloc[:, 1:x], y, ['knn'])\n",
    "    \n",
    "train_ranked_models(ranked_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_PCA(data, preds):\n",
    "    \n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Generate PCA results plot\n",
    "    pca_results = rs.pca_results(data, pca)\n",
    "    #print(pca_results)\n",
    "    pca_data = pca.transform(data)\n",
    "    \n",
    "    # Create a DataFrame for the reduced data\n",
    "    pca_data = pd.DataFrame(pca_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4'])\n",
    "    plot_data = pd.concat([preds, pca_data], axis = 1)\n",
    "    \n",
    "    # Scatterplotting the transformed data if it's 2 Dimensions\n",
    "    \"\"\"fig, ax = plt.subplots(figsize = (14,8))\n",
    "    cmap = cm.get_cmap('gist_rainbow')\n",
    "    \n",
    "    for i, cluster in plot_data.groupby('converted_goals'):   \n",
    "        cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \n",
    "                     color = cmap((i)*1.0/(4)), label = 'Points %i'%(i), s=30);\"\"\"\n",
    "    \n",
    "    return pca, plot_data, pca_results\n",
    "\n",
    "#(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals') \n",
    "\n",
    "pca, pca_data, pca_results = plot_PCA(ha_X, ha_y)\n",
    "#display(pca_data.head())\n",
    "\n",
    "print(pca_data.shape)\n",
    "# Runs models on PCA to see if there is a difference in the results\n",
    "models = run_features(pca_data, [], 'converted_goals', ['knn'])\n",
    "(pca_y, pca_X) = model_libs._extract_target(pca_data, 'converted_goals')\n",
    "knn = models[0]\n",
    "#preds = knn.predict(pca_X)\n",
    "\n",
    "actual_y = pd.DataFrame(pca_y.values, columns=['actual'])\n",
    "predictions = pd.DataFrame(knn.predict(pca_X), columns=['predictions'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(pca_results)\n",
    "print(type(pca_results))\n",
    "print(pca_results[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on Expected Goals Final Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals')\n",
    "knn = train_models(round_number, ha_X, ha_y, ['knn'])\n",
    "actual_y = pd.DataFrame(ha_y.values, columns=['actual'])\n",
    "\n",
    "predictions = pd.DataFrame(knn[0].predict(ha_X), columns=['expected_goal_category'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['expected_goal_category'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING LINEAR REGRESSION on FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Regression Model independent from Classification (without the Classifier's input as a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn import grid_search\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "\n",
    "\n",
    "display(regr_data.head())\n",
    "# Drop the original RPI features and everything else\n",
    "r_data = regr_data.drop(sub_cols + ignore_cols + ['round', 'converted_goals'] + ['rpi', 'opp_rpi'], axis=1)\n",
    "display(r_data.head())\n",
    "\n",
    "(regr_y, regr_X) = model_libs._extract_target(r_data, 'goals')\n",
    "        \n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(regr_X, regr_y, test_size=0.2, random_state=42)\n",
    "\n",
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X_train, y_train)\n",
    "#predicted = cross_val_predict(lin_regr, reg_X, reg_y, cv=10)\n",
    "print('Coefficients: \\n', lin_regr.coef_)\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((lin_regr.predict(X_test) - y_test) ** 2))\n",
    "\n",
    "preds = pd.DataFrame(lin_regr.predict(X_test), columns=['predictions'])\n",
    "preds['rounded'] = preds.loc[0:].apply(lambda r: np.round(r), axis=1)\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % lin_regr.score(X_test, preds['rounded']))\n",
    "display(preds.head())\n",
    "preds = pd.concat([preds, pd.DataFrame(y_test.as_matrix(), columns=['actual'])], axis = 1)\n",
    "preds['correct'] = preds.apply(lambda r: model_libs.predictions_diff(r['rounded'], r['actual']), axis=1)\n",
    "preds['correct_category'] = preds.apply(lambda r: model_libs.check_category(r['rounded'], r['actual']), axis=1)\n",
    "#display(preds.head())\n",
    "accuracy = np.divide(preds['correct'].sum(),float(len(preds['correct'])))\n",
    "accuracy_category = np.divide(preds['correct_category'].sum(), float(len(preds['correct_category'])))\n",
    "print('accuracy on X TEST')\n",
    "print(accuracy)\n",
    "print('accuracy_category on X TEST')\n",
    "print(accuracy_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting all goals on the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#k_X = pd.DataFrame(k_X)\n",
    "\n",
    "all_preds = pd.DataFrame(lin_regr.predict(regr_X), columns=['predictions'])\n",
    "all_preds['rounded'] = all_preds.loc[0:].apply(lambda r: np.round(r), axis=1)\n",
    "\n",
    "all_preds = pd.concat([regr_X, all_preds], axis = 1)\n",
    "\n",
    "all_preds = pd.concat([all_preds, regr_data['goals']], axis=1)\n",
    "\n",
    "necessary = all_preds[['predictions', 'rounded', 'goals']]\n",
    "    \n",
    "necessary['correct'] = necessary.apply(lambda r: model_libs.predictions_diff(r['rounded'], r['goals']), axis=1)\n",
    "necessary['correct_category'] = necessary.apply(lambda r: model_libs.check_category(r['predictions'], r['goals']), axis=1)\n",
    "display(necessary)\n",
    "\n",
    "accuracy = np.divide(necessary['correct'].sum(),float(len(necessary['correct'])))\n",
    "accuracy_category = np.divide(necessary['correct_category'].sum(), float(len(necessary['correct_category'])))\n",
    "\n",
    "print('accuracy on All of X')\n",
    "print(accuracy)\n",
    "print('accuracy_category on All of X')\n",
    "print(accuracy_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
