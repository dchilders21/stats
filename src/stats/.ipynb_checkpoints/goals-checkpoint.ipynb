{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIDE NOTE: would love to get any critiques, hints, tips, oberservations on any level not just on ML (but obviously ML as the priority).  Beginner Python learner (though I took the Full Stack Course) so any suggestions there are welcome.  Thanks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will need to import these libs plus setting some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZED...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import renders as rs\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.cm as cm\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "# Might need to change the path of the included libraries.\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/match_stats.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_model.py')\n",
    "sys.path.append('/anaconda/envs/stats/lib/python3.5/site-packages')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/model_libs.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_data.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats')\n",
    "# print(sys.path)\n",
    "os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "#print(os.getcwd())\n",
    "\n",
    "from stats import form_data, match_stats, model_libs, form_model, predict_matches\n",
    "\n",
    "# Variables\n",
    "round_number = 27 # for MLS only\n",
    "target_col = 'points'\n",
    "ignore_cols = ['games_played', 'match_id', 'team_id', 'team_name', 'opp_id', 'opp_name', 'scheduled']\n",
    "all_models = ['log', 'svc', 'gmm', 'knn', 'gnb']\n",
    "\n",
    "\"\"\" this variable 'testing' should be false if using CSV's and not pulling from the database. \"\"\"\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pulling in the data either from the Database or the CSV (CSV for testers).  Data has some added features in between the database and 'raw_data'.  Essentially trying 3 different versions of the data in this set.  One version contains all the features.  Another version has the standard features plus the calculated features of the home/away stats for a matchup.  The last version has the standard features plus the 'extended features' which not only includes the stats of the two teams in the match up but also the current teams previous opponents and the current teams opponents of the team opponents in that matchup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded...\n"
     ]
    }
   ],
   "source": [
    "data_csv = 'round' + str(round_number) + '.csv'\n",
    "\n",
    "if testing:\n",
    "    raw_data = form_data.run_data(round_number)\n",
    "    raw_data.to_csv(data_csv)\n",
    "    print(\"Raw Data Saved to CSV\")\n",
    "else:\n",
    "    #Reading in a CSV adds the first index column\n",
    "    raw_data = pd.read_csv(data_csv)\n",
    "    raw_data = raw_data.drop(raw_data.columns[[0]], axis=1)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)\n",
    "\n",
    "print('Data Loaded...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Functions that trains basic models (and saves them if need be).  'load_models' will  load exisiting models to save time in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removes Columns to Ignore and Splits the Target Column\n",
    "def split_target(data):\n",
    "    td = model_libs._clone_and_drop(data, ignore_cols)\n",
    "    (y, X) = model_libs._extract_target(td, target_col)\n",
    "    return X, y\n",
    "\n",
    "\"\"\"No Cross-Validation and No Tuning\"\"\"\n",
    "def train_models(round_num, X, y, models):\n",
    "    \n",
    "    if os.path.isdir(\"/models/\" + str(round_num)):\n",
    "        print('Making New Directory for the Round')\n",
    "        os.chdir('/Users/senzari/Machine_Learning/stats/src/models')\n",
    "        os.makedirs(str(round_num))\n",
    "        os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "        \n",
    "    finished_models = []\n",
    "    \n",
    "    for i in models:\n",
    "        \n",
    "        model_round = 'models/' + str(round_num) + '/' + str(i) + '_round_' + str(round_num) + '.pk1'\n",
    "        \n",
    "        \n",
    "        if i == 'log':\n",
    "            log = form_model.build_model(X, y, i)\n",
    "            joblib.dump(log, model_round)\n",
    "            finished_models.append(log)\n",
    "        elif i == 'svc':\n",
    "            svc = form_model.build_model(X, y, i)\n",
    "            joblib.dump(svc, model_round)\n",
    "            finished_models.append(svc)\n",
    "        elif i == 'gmm':\n",
    "            gmm = form_model.build_model(X, y, i)\n",
    "            joblib.dump(gmm, model_round)\n",
    "            finished_models.append(gmm)\n",
    "        elif i == 'knn':\n",
    "            kmeans = form_model.build_model(X, y, i)\n",
    "            joblib.dump(kmeans, model_round)\n",
    "            finished_models.append(kmeans)\n",
    "        elif i == 'gnb':\n",
    "            gnb = form_model.build_model(X, y, i)\n",
    "            joblib.dump(gnb, model_round)\n",
    "            finished_models.append(gnb)\n",
    "    \n",
    "    return finished_models\n",
    "        \n",
    "        \n",
    "def load_models(round_num):\n",
    "    \n",
    "    loaded_models = []\n",
    "    \n",
    "    for i in all_models:\n",
    "        model_round = 'models/' + str(round_num) + '/' + str(i) + '_round_' + str(round_num) + '.pk1'\n",
    "        if i == 'log':\n",
    "             log = joblib.load(model_round)\n",
    "             loaded_models.append(log)\n",
    "        if i == 'svc':\n",
    "             svc= joblib.load(model_round)\n",
    "             loaded_models.append(svc)\n",
    "        elif i == 'gmm':\n",
    "            gmm = joblib.load(model_round)\n",
    "            loaded_models.append(gmm)\n",
    "        elif i == 'knn':\n",
    "            kmeans = joblib.load(model_round)\n",
    "            loaded_models.append(kmeans)\n",
    "        elif i == 'gnb':\n",
    "            gnb = joblib.load(model_round)\n",
    "            loaded_models.append(gnb)\n",
    "        \n",
    "        print(\"Success :: Loaded - \" + str(i))\n",
    "        \n",
    "    return loaded_models\n",
    "\n",
    "# raw_X, raw_y = split_target(raw_data)\n",
    "\n",
    "# display(raw_data.head())\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "formatted_data = raw_data.drop('points', 1)\n",
    "\n",
    "def set_group(goals):\n",
    "    if goals >= 2:\n",
    "        return 1\n",
    "    elif goals < 2:\n",
    "        return 0\n",
    "    \n",
    "formatted_data['converted_goals'] = formatted_data.apply(lambda row: set_group(row['goals']), axis=1)\n",
    "\n",
    "all_data = formatted_data\n",
    "\n",
    "formatted_data = model_libs._clone_and_drop(formatted_data, ignore_cols)\n",
    "\n",
    "rf = model_libs.reformat_formation(formatted_data, formatted_data.groupby('current_formation').groups, formatted_data.groupby('opp_formation').groups)\n",
    "\n",
    "for r in range(len(rf)):\n",
    "    formatted_data = formatted_data.replace(rf[r], r)\n",
    "\n",
    "partially_formatted_data = formatted_data\n",
    "formatted_data = formatted_data.drop('goals', 1)\n",
    "\n",
    "(f_y, f_X) = model_libs._extract_target(formatted_data, 'converted_goals')\n",
    "    \n",
    "#display(formatted_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FORMATTING DATA AS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standard_cols = ['current_formation', 'is_home', 'avg_points', 'avg_goals_for', 'avg_goals_against', 'margin', 'goal_diff',\n",
    "                'win_percentage', 'sos', 'opp_is_home', 'opp_avg_points', 'opp_avg_goals', 'opp_margin', 'opp_goal_diff', 'opp_win_percentage',\n",
    "                'opp_opp_record', 'goals']\n",
    "# 17 in each\n",
    "home_cols = ['current_team_home_possession', 'current_team_home_attacks', 'current_team_home_dangerous_attacks', 'current_team_home_yellow_card',\n",
    "            'current_team_home_corner_kicks', 'current_team_home_shots_on_target', 'current_team_home_shots_total', 'current_team_home_ball_safe',\n",
    "            'current_team_home_played', 'current_opp_away_attacks', 'current_opp_away_dangerous_attacks', 'current_opp_away_yellow_card',\n",
    "            'current_opp_away_corner_kicks', 'current_opp_away_shots_on_target', 'current_opp_away_shots_total', 'current_opp_away_ball_safe',\n",
    "            'current_opp_away_played']\n",
    "away_cols = ['current_team_away_possession', 'current_team_away_attacks', 'current_team_away_dangerous_attacks', 'current_team_away_yellow_card', \n",
    "            'current_team_away_corner_kicks', 'current_team_away_shots_on_target', 'current_team_away_shots_total', 'current_team_away_ball_safe', \n",
    "            'current_team_away_played', 'current_opp_home_attacks', 'current_opp_home_dangerous_attacks', 'current_opp_home_yellow_card', 'current_opp_home_corner_kicks', \n",
    "             'current_opp_home_shots_on_target', 'current_opp_home_shots_total', 'current_opp_home_ball_safe', 'current_opp_home_played']\n",
    "\n",
    "extended_features = ['e_f_dangerous_attacks', 'e_f_shots_total', 'e_f_shots_on_target', 'e_f_ball_safe', 'e_f_possession', 'e_f_attacks',\n",
    "               'opp_e_f_dangerous_attacks', 'opp_e_f_shots_total', 'opp_e_f_shots_on_target', 'opp_e_f_ball_safe', 'opp_e_f_possession', 'opp_e_f_attacks',\n",
    "               'prev_opp_e_f_dangerous_attacks', 'prev_opp_e_f_shots_total', 'prev_opp_e_f_shots_on_target', 'prev_opp_e_f_ball_safe', 'prev_opp_e_f_possession', 'prev_opp_e_f_attacks',\n",
    "               'opp_opp_e_f_dangerous_attacks', 'opp_opp_e_f_shots_total', 'opp_opp_e_f_shots_on_target', 'opp_opp_e_f_ball_safe', 'opp_opp_e_f_possession', 'opp_opp_e_f_attacks']\n",
    "\n",
    "optional_extended_features = ['prev_opp_e_f_dangerous_attacks', 'prev_opp_e_f_shots_total', 'prev_opp_e_f_shots_on_target', 'prev_opp_e_f_ball_safe', 'prev_opp_e_f_possession', 'prev_opp_e_f_attacks',\n",
    "               'opp_opp_e_f_dangerous_attacks', 'opp_opp_e_f_shots_total', 'opp_opp_e_f_shots_on_target', 'opp_opp_e_f_ball_safe', 'opp_opp_e_f_possession', 'opp_opp_e_f_attacks']\n",
    "\n",
    "played_features = ['current_team_home_played', 'current_team_away_played','current_opp_home_played', 'current_opp_away_played', 'opp_is_home']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING CLASSIFICATION MODEL ON IF TEAMS SCORE 0-1 OR 1-2 ON GAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_features(data, drop_data, target, models):\n",
    "    \n",
    "    new_data = data.drop(drop_data, axis=1)\n",
    "    \n",
    "    display(new_data.head())\n",
    "    \n",
    "    (y, X) = model_libs._extract_target(new_data, target)\n",
    "    \n",
    "    models = train_models(round_number, X, y, models)\n",
    "    \n",
    "    return models\n",
    "\n",
    "\n",
    "#### Running ALL Features \n",
    "#models_test_1 = run_features(formatted_data, [], 'converted_goals', all_models)\n",
    "\n",
    "#### Running only STANDARD Features\n",
    "#models_test_2 = run_features(formatted_data, home_cols + away_cols + extended_features + played_features, 'converted_goals', all_models)\n",
    "\n",
    "#### Running STANDARD and EXTENDED Features\n",
    "#models_test_3 = run_features(formatted_data, home_cols + away_cols + optional_extended_features + played_features, 'converted_goals', all_models)\n",
    "\n",
    "# Creates new column list for merged columns\n",
    "merged_cols = []\n",
    "\n",
    "for cols in home_cols:\n",
    "    merged_cols.append(model_libs.rename_column(cols))\n",
    "    \n",
    "# Creates a Home and Away Table with Standard and Home/Away Columns.  Will merge together further down\n",
    "#ha_data = model_libs._clone_and_drop(formatted_data, ignore_cols)\n",
    "#display(reg_data.head())\n",
    "home_data = formatted_data.loc[formatted_data.loc[:, 'is_home'] == 1, standard_cols + home_cols + extended_features + ['converted_goals']]\n",
    "#display(home_data.head())\n",
    "away_data = formatted_data.loc[formatted_data.loc[:, 'is_home'] == 0, standard_cols + away_cols + extended_features + ['converted_goals']]\n",
    "\n",
    "# Combine the Home Table with the Away Table\n",
    "ha_data = home_data.append(away_data)\n",
    "\n",
    "ha_data = ha_data.drop('goals', axis=1)\n",
    "\n",
    "# Combine Home/Away Columns into one for Current Team and Opponent.  Cuts down on Features\n",
    "# Adds from whatever column that doesn't have Nan\n",
    "for c in range(len(merged_cols)):\n",
    "    ha_data[merged_cols[c]] = ha_data.apply(lambda row: model_libs.pick_column(row[home_cols[c]], row[away_cols[c]]), axis=1 )\n",
    "\n",
    "ha_data = ha_data.drop(home_cols + away_cols, axis=1)\n",
    "\n",
    "#### Running STANDARD and HOME and AWAY Features\n",
    "#models_test_4 = run_features(ha_data, extended_features + ['current_opp_played', 'current_team_played'], 'converted_goals', ['log'])\n",
    "\n",
    "\n",
    "models_test_1 = run_features(formatted_data, [], 'converted_goals', ['log'])\n",
    "(f_y, f_X) = model_libs._extract_target(formatted_data, 'converted_goals')\n",
    "log = models_test_1[0]\n",
    "print(log.predict_proba(f_X))\n",
    "\n",
    "actual_y = pd.DataFrame(f_y.values, columns=['actual'])\n",
    "predictions = pd.DataFrame(log.predict(f_X), columns=['predictions'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "display(preds.head())\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since STANDARD and HOME and AWAY Features got the best score going to optimize and minimize off of those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_RFECV(X, y):\n",
    "    svc = SVC(kernel=\"linear\")\n",
    "    rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(y, 2),\n",
    "                  scoring='accuracy')\n",
    "    rfecv.fit(X, y)\n",
    "\n",
    "    # Plot number of features VS. cross-validation scores\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Number of features selected\")\n",
    "    plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "    plt.show()\n",
    "    \n",
    "plot_RFECV(ha_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_RFE(X, y):\n",
    "    # Create the RFE object and rank each pixel\n",
    "    svc = SVC(kernel=\"linear\", C=1)\n",
    "    rfe = RFE(estimator=svc, n_features_to_select=1, step=1)\n",
    "    rfe.fit(X, y)\n",
    "    print(rfe.ranking_)\n",
    "    return rfe.ranking_\n",
    "\n",
    "(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals')\n",
    "\n",
    "# Taking all of Home/Away Data (even the Extended Features)\n",
    "rankings = create_RFE(ha_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Reordering the columns in the data to reflect the rankings \"\"\"\n",
    "\n",
    "rankings_name = []\n",
    "ranked_X = ha_X\n",
    "\n",
    "for r in range(len(rankings)):\n",
    "    column_name = ha_X.iloc[:, rankings[r]-1].name\n",
    "    rankings_name.append(column_name)\n",
    "\n",
    "ranked_X = ranked_X.reindex_axis(rankings_name, axis=1)\n",
    "display(ranked_X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_ranked_models(X, y):\n",
    "    \n",
    "    for x in xrange(10, 20):\n",
    "        print('X :: {}'.format(x))\n",
    "        ranked_models = train_models(round_number, X.iloc[:, 1:x], y, ['knn'])\n",
    "    \n",
    "train_ranked_models(ranked_X, ha_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_PCA(data, preds):\n",
    "    \n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(data)\n",
    "    \n",
    "    # Generate PCA results plot\n",
    "    pca_results = rs.pca_results(data, pca)\n",
    "    #print(pca_results)\n",
    "    pca_data = pca.transform(data)\n",
    "    \n",
    "    # Create a DataFrame for the reduced data\n",
    "    pca_data = pd.DataFrame(pca_data, columns = ['Dimension 1', 'Dimension 2', 'Dimension 3', 'Dimension 4'])\n",
    "    plot_data = pd.concat([preds, pca_data], axis = 1)\n",
    "    \n",
    "    # Scatterplotting the transformed data if it's 2 Dimensions\n",
    "    \"\"\"fig, ax = plt.subplots(figsize = (14,8))\n",
    "    cmap = cm.get_cmap('gist_rainbow')\n",
    "    \n",
    "    for i, cluster in plot_data.groupby('converted_goals'):   \n",
    "        cluster.plot(ax = ax, kind = 'scatter', x = 'Dimension 1', y = 'Dimension 2', \n",
    "                     color = cmap((i)*1.0/(4)), label = 'Points %i'%(i), s=30);\"\"\"\n",
    "    \n",
    "    return pca, plot_data, pca_results\n",
    "\n",
    "#(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals') \n",
    "\n",
    "pca, pca_data, pca_results = plot_PCA(ha_X, ha_y)\n",
    "#display(pca_data.head())\n",
    "\n",
    "print(pca_data.shape)\n",
    "# Runs models on PCA to see if there is a difference in the results\n",
    "models = run_features(pca_data, [], 'converted_goals', ['knn'])\n",
    "(pca_y, pca_X) = model_libs._extract_target(pca_data, 'converted_goals')\n",
    "knn = models[0]\n",
    "#preds = knn.predict(pca_X)\n",
    "\n",
    "actual_y = pd.DataFrame(pca_y.values, columns=['actual'])\n",
    "predictions = pd.DataFrame(knn.predict(pca_X), columns=['predictions'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(pca_results)\n",
    "print(type(pca_results))\n",
    "print(pca_results[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions on Final Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(ha_y, ha_X) = model_libs._extract_target(ha_data, 'converted_goals')\n",
    "knn = train_models(round_number, ha_X, ha_y, ['knn'])\n",
    "actual_y = pd.DataFrame(ha_y.values, columns=['actual'])\n",
    "\n",
    "predictions = pd.DataFrame(knn[0].predict(ha_X), columns=['expected_goal_category'])\n",
    "preds = pd.concat([predictions, actual_y], axis=1)\n",
    "preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['expected_goal_category'], r['actual']), axis=1)\n",
    "accuracy = np.divide(preds['diff'].sum(),float(len(preds['diff'])))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING LINEAR REGRESSION on FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Regression Model independent from Classification (without the Classifier's input as a feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-2237c74b8d61>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-2237c74b8d61>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    regr_data.set_value(:,regr_data['opp_converted_goals'], \"\")\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Can't use formatted_data.  Still need the goals column\n",
    "regr_data = all_data[(all_data['team_id'] == 21) | (all_data['opp_id'] == 21)]\n",
    "#display(regr_data)\n",
    "#regr_data.loc[regr_data['opp_converted_goals']] = \"\"\n",
    "g = regr_data.groupby(['match_id'])\n",
    "groups = dict(list(g))\n",
    "for g in groups:\n",
    "    print(' ========== ')\n",
    "    print(g)\n",
    "    teams = np.array(groups[g]['team_id'].values)\n",
    "    opps = np.array(groups[g]['opp_id'].values)\n",
    "    for t, o in zip(teams, opps):    \n",
    "        print('t : {}, o : {}').format(t, o)\n",
    "        match = regr_data[(regr_data['team_id'] == t) & (regr_data['opp_id'] == o)]\n",
    "        match_id = match['match_id']\n",
    "        curr_goals = match['converted_goals']\n",
    "        reverse_match = regr_data[(regr_data['team_id'] == o) & (regr_data['opp_id'] == t)]\n",
    "        reverse_match = reverse_match[reverse_match['match_id'] == match_id.values[0]]\n",
    "        regr_data.loc[reverse_match.index.values[0], 'opp_converted_goals'] = curr_goals.values[0]\n",
    "        \n",
    "pd.set_option('display.max_rows', 5000)     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr_data.to_csv('converted_goals.csv')\n",
    "display(regr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split, cross_val_predict\n",
    "from sklearn import linear_model\n",
    "from sklearn import grid_search\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 5000)\n",
    "\n",
    "# Can't use formatted_data.  Still need the goals column\n",
    "regr_data = all_data\n",
    "\n",
    "# Need to add results of the classifier to the features\n",
    "regr_data = pd.concat([regr_data, predictions], axis=1)\n",
    "display(regr_data.head())\n",
    "rd = regr_data.groupby('current_formation')\n",
    "print(rd)\n",
    "\n",
    "\n",
    "# Both for current team and the current opponent\n",
    "regr_data = model_libs._clone_and_drop(regr_data, home_cols + away_cols + extended_features + ignore_cols + ['points'])\n",
    "\n",
    "# What other data can we remove that's not useful to predicting the number of goals per game\n",
    "regr_data = regr_data.drop(['converted_goals'], axis=1)\n",
    "\n",
    "(regr_y, regr_X) = model_libs._extract_target(regr_data, 'goals')\n",
    "\n",
    "# Input X must be non-negative for chi-best one of the feautre selections. So will need to scale for this\n",
    "# scaled_X = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(regr_X))\n",
    "\n",
    "# Removes features with not enough variance - 80% is the threshold\n",
    "#sel = VarianceThreshold(threshold = (.8 * (1 - .8)))\n",
    "#sel_X = pd.DataFrame(sel.fit_transform(regr_X))\n",
    "\n",
    "#display(regr_X.head())\n",
    "\n",
    "# Selects the best features for regression\n",
    "# k_X = SelectKBest(f_regression, k=20).fit_transform(regr_X, regr_y)\n",
    "# display(pd.DataFrame(k_X))\n",
    "        \n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(regr_X, regr_y, test_size=0.2, random_state=42)\n",
    "\n",
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X_train, y_train)\n",
    "#predicted = cross_val_predict(lin_regr, reg_X, reg_y, cv=10)\n",
    "print('Coefficients: \\n', lin_regr.coef_)\n",
    "print(\"Residual sum of squares: %.2f\"\n",
    "      % np.mean((lin_regr.predict(X_test) - y_test) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % lin_regr.score(X_test, preds['rounded']))\n",
    "\n",
    "\"\"\"regressor = DecisionTreeRegressor()\n",
    "parameters = {'max_depth':(1,20,30,40,50,60,70,80,90,100), 'max_features': (2, 5, 10, 15, 20)}\n",
    "dec_regr = grid_search.GridSearchCV(regressor, parameters, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "#print(cross_val_score(dec_regr, k_X, regr_y, cv=10))\n",
    "print(dec_regr)\n",
    "dec_regr.fit(X_train, y_train)\"\"\"\n",
    "\n",
    "preds = pd.DataFrame(lin_regr.predict(X_test), columns=['predictions'])\n",
    "preds['rounded'] = preds.loc[0:].apply(lambda r: np.round(r), axis=1)\n",
    "display(preds.head())\n",
    "preds = pd.concat([preds, pd.DataFrame(y_test.as_matrix(), columns=['actual'])], axis = 1)\n",
    "preds['correct'] = preds.apply(lambda r: model_libs.predictions_diff(r['rounded'], r['actual']), axis=1)\n",
    "preds['correct_category'] = preds.apply(lambda r: model_libs.check_category(r['rounded'], r['actual']), axis=1)\n",
    "#display(preds.head())\n",
    "accuracy = np.divide(preds['correct'].sum(),float(len(preds['correct'])))\n",
    "accuracy_category = np.divide(preds['correct_category'].sum(), float(len(preds['correct_category'])))\n",
    "print('accuracy on X TEST')\n",
    "print(accuracy)\n",
    "print('accuracy_category on X TEST')\n",
    "print(accuracy_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting all goals on the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k_X = pd.DataFrame(k_X)\n",
    "\n",
    "all_preds = pd.DataFrame(lin_regr.predict(k_X), columns=['predictions'])\n",
    "all_preds['rounded'] = all_preds.loc[0:].apply(lambda r: np.round(r), axis=1)\n",
    "\n",
    "all_preds = pd.concat([k_X, all_preds], axis = 1)\n",
    "\n",
    "all_preds = pd.concat([all_preds, regr_data['goals']], axis=1)\n",
    "\n",
    "necessary = all_preds[['predictions', 'rounded', 'goals']]\n",
    "    \n",
    "necessary['correct'] = necessary.apply(lambda r: model_libs.predictions_diff(r['rounded'], r['goals']), axis=1)\n",
    "necessary['correct_category'] = necessary.apply(lambda r: model_libs.check_category(r['predictions'], r['goals']), axis=1)\n",
    "display(necessary)\n",
    "\n",
    "accuracy = np.divide(necessary['correct'].sum(),float(len(necessary['correct'])))\n",
    "accuracy_category = np.divide(necessary['correct_category'].sum(), float(len(necessary['correct_category'])))\n",
    "\n",
    "print('accuracy on All of X')\n",
    "print(accuracy)\n",
    "print('accuracy_category on All of X')\n",
    "print(accuracy_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "Use these models trained on the optimal data to predict upcoming matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find upcoming matches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#upcoming_matches, match_details = predict_matches.get_upcoming_matches()\n",
    "#upcoming_matches.to_csv('upcoming_matches.csv')\n",
    "#match_details.to_csv('match_details.csv')\n",
    "#print(\"Upcoming Data Saved to CSV\")\n",
    "\n",
    "upcoming_matches = pd.read_csv('upcoming_matches.csv')\n",
    "upcoming_matches = upcoming_matches.drop(upcoming_matches.columns[[0]], axis=1)\n",
    "\n",
    "match_details = pd.read_csv('match_details.csv')\n",
    "match_details = match_details.drop(match_details.columns[[0]], axis=1)\n",
    "\n",
    "display(upcoming_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on upcoming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stats import predict_matches\n",
    "\n",
    "for i in models:\n",
    "    if i == 'svc':\n",
    "        svc_preds, upcoming_data = predict_matches.predictions(upcoming_matches, match_details, svc)\n",
    "        print(svc_preds)\n",
    "    elif i == 'gmm':\n",
    "        gmm_preds, upcoming_data = predict_matches.predictions(upcoming_matches, match_details, gmm)\n",
    "        print(gmm_preds)\n",
    "    elif i == 'knn':\n",
    "        knn_preds, upcoming_data = predict_matches.predictions(upcoming_matches, match_details, knn)\n",
    "        print(knn_preds)\n",
    "    elif i == 'gnb':\n",
    "        gnb_preds, upcoming_data = predict_matches.predictions(upcoming_matches, match_details, gnb)\n",
    "        print(gnb_preds)\n",
    "        \n",
    "display(upcoming_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output those predictions to a CSV so easier to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['team_name', 'opp_name', 'scheduled']\n",
    "# Remove all columns except the ones above\n",
    "upcoming_matches = upcoming_data[columns]\n",
    "\n",
    "# Add predictions to the end of that DF\n",
    "results = pd.DataFrame({'SVC': svc_preds, 'GMM': gmm_preds, 'KNN': knn_preds, 'GNB': gnb_preds})\n",
    "upcoming_matches = upcoming_matches.join(results)\n",
    "reordered_matches = pd.DataFrame([])\n",
    "\n",
    "for rows in upcoming_matches.iterrows():\n",
    "    for i in upcoming_matches['team_name']:\n",
    "        if rows[1]['opp_name'] == i:\n",
    "            reordered_matches = reordered_matches.append(rows[1])\n",
    "            reordered_matches = reordered_matches.append(upcoming_matches[upcoming_matches['team_name'].isin([i])])\n",
    "\n",
    "reordered_matches = reordered_matches.drop_duplicates() \n",
    "columns = ['scheduled', 'team_name', 'opp_name', 'SVC', 'KNN', 'GMM', 'GNB']\n",
    "reordered_matches = reordered_matches[columns]\n",
    "# upcoming_matches = upcoming_matches[(upcoming_matches['scheduled'] < '2016-08-26')]\n",
    "reordered_matches.to_csv('predictions_' + str(round_number) + '.csv')\n",
    "print('Prediction CSV saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESULTS\n",
    "\n",
    "Excluding GMM...\n",
    "\n",
    "FC Dallas vs Portland Timbers :: 3-1 (Unanimously Dallas)\n",
    "New England Revolution vs Colorado Rapids :: 2-0 (Mixed, 1 model picked NER to win)\n",
    "New York City FC vs DC United :: 3-2 (Unanimously NYCFC)\n",
    "Chicago Fire vs Philadelphia Union :: 3-0 (2 models picked Chi and 1 picked Phi)\n",
    "Vancouver Whitecaps vs New York Red Bulls :: 0-1 (WRONG, 2 models picked Vancouver to win)\n",
    "LA Galaxy vs Columbus Crew :: 2-1 (2 models picked LA to win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "My Questions:\n",
    "- For GMM, how can we tell what classification it's predicting.  If you notice in the predictions CSV it says 0, 1, 2.  How do I know which one of those are win, draw, or a loss (3, 1, 0)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
