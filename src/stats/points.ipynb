{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZED...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import renders as rs\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.cm as cm\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "# Might need to change the path of the included libraries.\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/match_stats.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_model.py')\n",
    "sys.path.append('/anaconda/envs/stats/lib/python3.5/site-packages')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/model_libs.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats/form_data.py')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats')\n",
    "sys.path.append('/Users/senzari/Machine_Learning/stats/src/stats')\n",
    "# print(sys.path)\n",
    "os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "#print(os.getcwd())\n",
    "\n",
    "from stats import form_data, match_stats, model_libs, form_model, predict_matches\n",
    "\n",
    "# Variables\n",
    "round_number = 27 # for MLS only\n",
    "target_col = 'points'\n",
    "ignore_cols = ['match_id', 'team_id', 'team_name', 'opp_id', 'opp_name', 'scheduled', 'games_played', 'round']\n",
    "sub_cols = ['current_formation', 'avg_goals_against', 'goal_diff', 'win_percentage', 'sos',\n",
    "           'opp_win_percentage', 'opp_sos', 'current_team_yellow_cards', 'current_team_corner_kicks', 'current_team_first_half_goals', 'current_team_sec_half_goals', \n",
    "           'opp_team_yellow_cards', 'opp_team_corner_kicks', 'opp_team_first_half_goals', 'opp_team_sec_half_goals']\n",
    "\n",
    "all_models = ['log', 'svc', 'gmm', 'knn', 'gnb', 'randomForest']\n",
    "\n",
    "\"\"\" this variable 'testing' should be false if using CSV's and not pulling from the database. \"\"\"\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pulling in the data either from the Database or the CSV (CSV for testers). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_csv = 'raw_data.csv'\n",
    "\n",
    "if testing:\n",
    "    raw_data = form_data.run_data()\n",
    "    raw_data.to_csv(data_csv)\n",
    "    print(\"Raw Data Saved to CSV\")\n",
    "else:\n",
    "    #Reading in a CSV adds the first index column\n",
    "    raw_data = pd.read_csv(data_csv)\n",
    "    raw_data = raw_data.drop(raw_data.columns[[0]], axis=1)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)\n",
    "print('Data Loaded...')\n",
    "print(\"Dataset size :: {}\".format(raw_data.shape))\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMATTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Function - Removes Columns to Ignore and Splits the Target Column\n",
    "def split_target(data):\n",
    "    td = model_libs._clone_and_drop(data, ignore_cols)\n",
    "    (y, X) = model_libs._extract_target(td, target_col)\n",
    "    return X, y\n",
    "\n",
    "\"\"\" Need to do some formatting of the Data before we run the models\"\"\"\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "rankings_data = raw_data.copy()\n",
    "\n",
    "\"\"\" Setting the RPI Quartiles on the raw data \"\"\"\n",
    "'''leagues = [\"USA\", \"ENG\", \"DEU\", \"ESP\", \"FRA\"]\n",
    "teams = form_data.get_teams()              \n",
    "rankings_data = model_libs.convert_sos_rpi(leagues, rankings_data, teams)\n",
    "rankings_data = rankings_data.drop(['rpi', 'opp_rpi'], axis=1)\n",
    "display(rankings_data.head(1))'''\n",
    "\n",
    "rankings_data[\"offensive_ranking\"] = pd.Series(None, index=rankings_data.index)\n",
    "rankings_data[\"opp_defensive_ranking\"] = pd.Series(None, index=rankings_data.index)\n",
    "\n",
    "leagues = model_libs.get_leagues_country_codes()\n",
    "#leagues = { \"epl\": 'ENG' }\n",
    "teams = form_data.get_teams()\n",
    "league_rounds = model_libs.get_leagues_rounds()\n",
    "test = False\n",
    "if test:\n",
    "    \"\"\" Going through each League\"\"\"\n",
    "    for key, value in leagues.iteritems():\n",
    "        print(key)\n",
    "        country_code = leagues[key]\n",
    "        round_num = league_rounds[key]\n",
    "        #round_num = 6\n",
    "        teams_in_league = teams[teams[\"country_code\"] == country_code]\n",
    "        \"\"\" Looping through the Rounds \"\"\"\n",
    "        for i in range(4, round_num):\n",
    "            print(\"ROUND :: {} \".format(i))\n",
    "            offensive_rankings = form_data.get_rankings(teams_in_league, i, \"offensive\", False)\n",
    "            rankings = model_libs.quartile_list(offensive_rankings, True)\n",
    "            offensive_rankings[\"offensive_rankings_quartiled\"] = rankings\n",
    "            print(\"Finished with Offensive Rankings\")\n",
    "            #print(offensive_rankings)\n",
    "\n",
    "            defensive_rankings = form_data.get_rankings(teams_in_league, i, \"defensive\", False)\n",
    "            rankings = model_libs.quartile_list(defensive_rankings, False)\n",
    "            defensive_rankings[\"defensive_rankings_quartiled\"] = rankings\n",
    "            print(\"Finished with Defensive Rankings\")\n",
    "            #print(defensive_rankings)\n",
    "\n",
    "            \"\"\" Loop through each Team in the League for that round and assign an Offensive Rank \"\"\"\n",
    "            for key, team in teams_in_league.iterrows():\n",
    "                \n",
    "                ''' If the team is the team_id then put in their offensive ranking for that game '''\n",
    "                offensive_rank = offensive_rankings.loc[offensive_rankings[0] == team['id'], \"offensive_rankings_quartiled\"]\n",
    "                idx = rankings_data.loc[(rankings_data[\"team_id\"] == team[\"id\"]) \n",
    "                        & (rankings_data[\"round\"] == (i)), \"offensive_ranking\"].index\n",
    "\n",
    "                rankings_data.set_value(idx, \"offensive_ranking\", offensive_rank.values[0])\n",
    "                ''' If the team is the opp then put in their defensive ranking for that game '''\n",
    "                defensive_rank = defensive_rankings.loc[defensive_rankings[0] == team['id'], \"defensive_rankings_quartiled\"]\n",
    "                opp_idx = rankings_data.loc[(rankings_data[\"opp_id\"] == team[\"id\"]) \n",
    "                        & (rankings_data[\"round\"] == (i))].index\n",
    "\n",
    "                rankings_data.set_value(opp_idx, \"opp_defensive_ranking\", defensive_rank.values[0])\n",
    "                \n",
    "                rankings_data.to_csv('rankings_data.csv')\n",
    "                \n",
    "else:\n",
    "    \n",
    "    rankings_data = pd.read_csv('rankings_data.csv')\n",
    "    rankings_data = rankings_data.drop(rankings_data.columns[[0]], axis=1)\n",
    "    \n",
    "print('Data Loaded...')\n",
    "                          \n",
    "\"\"\" Formatting data to convert goals scored to the correct category\"\"\"\n",
    "# Not using points as a target for this version, using goals\n",
    "rankings_data = rankings_data.drop('goals', 1)\n",
    "\n",
    "#rankings_data['converted_goals'] = rankings_data.apply(lambda row: model_libs.set_group(row['goals']), axis=1)\n",
    "\n",
    "rankings_data = rankings_data.drop(ignore_cols + ['current_formation'], 1)\n",
    "display(rankings_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rankings_data.to_csv('rankings_fully_formatted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rankings_data = pd.read_csv('rankings_fully_formatted.csv')\n",
    "rankings_data = rankings_data.drop(rankings_data.columns[[0]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUNNING CLASSIFICATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Training K Neighbors Classifier Model\n",
      "KNN Score on Training Set :: 0.694280078895\n",
      "KNN Score on Test Set:: 0.338582677165\n",
      "Finished K-Means Modeling\n",
      "0.623028391167\n"
     ]
    }
   ],
   "source": [
    "def run_features(data, drop_data, target, models):\n",
    "    \n",
    "    new_data = data.drop(drop_data, axis=1)\n",
    "    \n",
    "    #display(new_data.head())\n",
    "    \n",
    "    (y, X) = model_libs._extract_target(new_data, target)\n",
    "    \n",
    "    models = form_model.train_models(round_number, X, y, models)\n",
    "    \n",
    "    return models\n",
    "\n",
    "rankings_data = rankings_data.drop(['rpi', 'opp_rpi'], 1)\n",
    "\n",
    "#### Running ALL Features \n",
    "models_test_1 = run_features(rankings_data, [], 'points', [\"knn\"])\n",
    "\n",
    "(rankings_y, rankings_X) = model_libs._extract_target(rankings_data, 'points')\n",
    "\n",
    "def check_accuracy(model, data_X):\n",
    "    actual_y = pd.DataFrame(rankings_y.values, columns=['actual'])\n",
    "    predictions = pd.DataFrame(model.predict(data_X), columns=['predictions'])\n",
    "    preds = pd.concat([predictions, actual_y], axis=1)\n",
    "    preds['diff'] = preds.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "    accuracy = np.divide(preds['diff'].sum(), float(len(preds['diff'])))\n",
    "    print(accuracy)\n",
    "\n",
    "for m in models_test_1:\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_tuned_models(round_num, X, y):\n",
    "    for i in all_models:\n",
    "        models = form_model.build_tuned_model(X, y, i)\n",
    "        \n",
    "    return models\n",
    "        \n",
    "tuned_models = train_tuned_models(round_number, classifier_X, classifier_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_model = form_model.build_tuned_model(rankings_X, rankings_y, 'log')\n",
    "\n",
    "for m in log_model:\n",
    "    print(m)\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_forest_model = form_model.build_tuned_model(rankings_X, rankings_y, 'randomForest')\n",
    "\n",
    "for m in random_forest_model:\n",
    "    print(m)\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_model = form_model.build_tuned_model(rankings_X, rankings_y, 'knn')\n",
    "\n",
    "for m in knn_model:\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnb_model = form_model.build_tuned_model(rankings_X, rankings_y, 'gnb')\n",
    "\n",
    "for m in gnb_model:\n",
    "    print(m)\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Tuning SVC Model\n",
      "[ 0.36538462  0.38461538  0.48076923  0.46153846  0.41176471  0.54901961\n",
      "  0.44        0.36734694  0.51020408  0.55102041]\n",
      "Accuracy: 0.45 (+/- 0.13)\n",
      "Finished SVC Modeling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index([u'is_home', u'goals_for', u'goals_allowed', u'opp_goals_for',\n",
       "       u'opp_goals_allowed', u'goal_efficiency',\n",
       "       u'opp_defensive_goal_efficiency', u'ratio_of_attacks',\n",
       "       u'opp_ratio_of_attacks', u'ratio_ball_safe_to_dangerous_attacks',\n",
       "       u'opp_ratio_ball_safe_to_dangerous_attacks', u'offensive_ranking',\n",
       "       u'opp_defensive_ranking'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=None, error_score='raise',\n",
      "       estimator=SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.08, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False),\n",
      "       fit_params={}, iid=True, n_jobs=1,\n",
      "       param_grid=[{'kernel': ['rbf'], 'C': [1, 10, 100]}],\n",
      "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)\n",
      "0.572555205047\n"
     ]
    }
   ],
   "source": [
    "svc_model = form_model.build_tuned_model(rankings_X, rankings_y, 'svc')\n",
    "\n",
    "display(rankings_X.columns)\n",
    "for m in svc_model:\n",
    "    print(m)\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gmm_model = form_model.build_tuned_model(rankings_X, rankings_y, 'gmm')\n",
    "\n",
    "for m in gmm_model:\n",
    "    print(m)\n",
    "    check_accuracy(m, rankings_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validating the SVC model with the PCA data to help prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success :: Loaded - knn\n",
      "Success :: Loaded - svc\n",
      "Success :: Loaded - randomForest\n",
      "Success :: Loaded - gnb\n",
      "Success :: Loaded - gmm\n",
      "Success :: Loaded - log\n"
     ]
    }
   ],
   "source": [
    "prediction_models = form_model.load_models(['knn', 'svc', 'randomForest', 'gnb', 'gmm', 'log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upcoming matches\n",
      "Data Loaded and Predicted...\n",
      "Added Rankings to Upcoming Matches\n"
     ]
    }
   ],
   "source": [
    "print('Upcoming matches')\n",
    "#upcoming_matches, match_details = predict_matches.get_upcoming_matches()\n",
    "#upcoming_matches.to_csv('upcoming_matches.csv')\n",
    "upcoming_matches = pd.read_csv('upcoming_matches.csv')\n",
    "upcoming_matches = upcoming_matches.drop(upcoming_matches.columns[[0]], axis=1)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "upcoming_data = predict_matches.predictions(upcoming_matches)\n",
    "print('Data Loaded and Predicted...')\n",
    "\n",
    "\"\"\" Setting the RPI Quartiles on the raw data \"\"\"\n",
    "\"\"\"leagues = [\"USA\", \"ENG\", \"DEU\", \"ESP\", \"FRA\"]\n",
    "teams = form_data.get_teams()              \n",
    "upcoming_data = model_libs.convert_sos_rpi(leagues, upcoming_data, teams)\n",
    "upcoming_data = upcoming_data.drop(['rpi', 'opp_rpi'], axis=1)\"\"\"\n",
    "\n",
    "upcoming_data[\"offensive_ranking\"] = pd.Series(None, index=rankings_data.index)\n",
    "upcoming_data[\"opp_defensive_ranking\"] = pd.Series(None, index=rankings_data.index)\n",
    "\n",
    "leagues = model_libs.get_leagues_country_codes()\n",
    "teams = form_data.get_teams()\n",
    "league_rounds = model_libs.get_leagues_rounds()\n",
    "test = False\n",
    "if test:\n",
    "    \"\"\" Going through each League\"\"\"\n",
    "    for key, value in leagues.iteritems():\n",
    "        print(key)\n",
    "        country_code = leagues[key]\n",
    "        round_num = league_rounds[key]\n",
    "        teams_in_league = teams[teams[\"country_code\"] == country_code]\n",
    "        \n",
    "        print(\"ROUND :: {} \".format(round_num))\n",
    "        offensive_rankings = form_data.get_rankings(teams_in_league, round_num, \"offensive\", True)\n",
    "        rankings = model_libs.quartile_list(offensive_rankings, True)\n",
    "        offensive_rankings[\"offensive_rankings_quartiled\"] = rankings\n",
    "        #print(offensive_rankings)\n",
    "        print(\"Finished with Offensive Rankings\")\n",
    "\n",
    "        defensive_rankings = form_data.get_rankings(teams_in_league, round_num, \"defensive\", True)\n",
    "        rankings = model_libs.quartile_list(defensive_rankings, False)\n",
    "        defensive_rankings[\"defensive_rankings_quartiled\"] = rankings\n",
    "        #print(defensive_rankings)\n",
    "        print(\"Finished with Defensive Rankings\")\n",
    "\n",
    "        \"\"\" Loop through each Team in the League for that round and assign an Offensive Rank \"\"\"\n",
    "        for key, team in teams_in_league.iterrows():\n",
    "                \n",
    "            ''' If the team is the team_id then put in their offensive ranking for that game '''\n",
    "            offensive_rank = offensive_rankings.loc[offensive_rankings[0] == team['id'], \"offensive_rankings_quartiled\"]\n",
    "            idx = upcoming_data.loc[(upcoming_data[\"team_id\"] == team[\"id\"]) \n",
    "                    & (upcoming_data[\"round\"] == (round_num)), \"offensive_ranking\"].index\n",
    "            upcoming_data.set_value(idx, \"offensive_ranking\", offensive_rank.values[0])\n",
    "            \n",
    "            ''' If the team is the opp then put in their defensive ranking for that game '''\n",
    "            defensive_rank = defensive_rankings.loc[defensive_rankings[0] == team['id'], \"defensive_rankings_quartiled\"]\n",
    "            opp_idx = upcoming_data.loc[(upcoming_data[\"opp_id\"] == team[\"id\"]) \n",
    "                    & (upcoming_data[\"round\"] == (round_num))].index\n",
    "\n",
    "            upcoming_data.set_value(opp_idx, \"opp_defensive_ranking\", defensive_rank.values[0])\n",
    "                \n",
    "        upcoming_data.to_csv('upcoming_formatted_matches.csv')\n",
    "                \n",
    "else:\n",
    "    \n",
    "    upcoming_data = pd.read_csv('upcoming_formatted_matches.csv')\n",
    "    upcoming_data = upcoming_data.drop(upcoming_data.columns[[0]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "print('Added Rankings to Upcoming Matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'is_home', u'goals_for', u'goals_allowed', u'opp_goals_for',\n",
       "       u'opp_goals_allowed', u'goal_efficiency',\n",
       "       u'opp_defensive_goal_efficiency', u'ratio_of_attacks',\n",
       "       u'opp_ratio_of_attacks', u'ratio_ball_safe_to_dangerous_attacks',\n",
       "       u'opp_ratio_ball_safe_to_dangerous_attacks', u'offensive_ranking',\n",
       "       u'opp_defensive_ranking'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_home</th>\n",
       "      <th>goals_for</th>\n",
       "      <th>goals_allowed</th>\n",
       "      <th>opp_goals_for</th>\n",
       "      <th>opp_goals_allowed</th>\n",
       "      <th>goal_efficiency</th>\n",
       "      <th>opp_defensive_goal_efficiency</th>\n",
       "      <th>ratio_of_attacks</th>\n",
       "      <th>opp_ratio_of_attacks</th>\n",
       "      <th>ratio_ball_safe_to_dangerous_attacks</th>\n",
       "      <th>opp_ratio_ball_safe_to_dangerous_attacks</th>\n",
       "      <th>offensive_ranking</th>\n",
       "      <th>opp_defensive_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.541528</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>1.264706</td>\n",
       "      <td>0.970370</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.896552</td>\n",
       "      <td>0.482993</td>\n",
       "      <td>0.591973</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.66666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.561934</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>1.153310</td>\n",
       "      <td>0.924012</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.490506</td>\n",
       "      <td>0.924012</td>\n",
       "      <td>1.029316</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>0.33333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.422713</td>\n",
       "      <td>0.505119</td>\n",
       "      <td>1.096886</td>\n",
       "      <td>1.042705</td>\n",
       "      <td>0.33333</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_home  goals_for  goals_allowed  opp_goals_for  opp_goals_allowed  \\\n",
       "0        0          5              4              3                  5   \n",
       "1        0          1              2              3                  3   \n",
       "2        1          3              5              5                  4   \n",
       "3        1          6              7              6                  4   \n",
       "4        0          2              6              5                  5   \n",
       "\n",
       "   goal_efficiency  opp_defensive_goal_efficiency  ratio_of_attacks  \\\n",
       "0         0.128205                       0.782609          0.541528   \n",
       "1         0.041667                       0.896552          0.482993   \n",
       "2         0.088235                       0.800000          0.561934   \n",
       "3         0.214286                       0.888889          0.480263   \n",
       "4         0.064516                       0.791667          0.422713   \n",
       "\n",
       "   opp_ratio_of_attacks  ratio_ball_safe_to_dangerous_attacks  \\\n",
       "0              0.396947                              1.264706   \n",
       "1              0.591973                              1.050000   \n",
       "2              0.480263                              1.153310   \n",
       "3              0.490506                              0.924012   \n",
       "4              0.505119                              1.096886   \n",
       "\n",
       "   opp_ratio_ball_safe_to_dangerous_attacks  offensive_ranking  \\\n",
       "0                                  0.970370            1.00000   \n",
       "1                                  1.150000            0.00000   \n",
       "2                                  0.924012            0.33333   \n",
       "3                                  1.029316            0.33333   \n",
       "4                                  1.042705            0.33333   \n",
       "\n",
       "   opp_defensive_ranking  \n",
       "0                1.00000  \n",
       "1                0.66666  \n",
       "2                1.00000  \n",
       "3                0.33333  \n",
       "4                1.00000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\" Need to remove the same columns from the data the same way we did on the raw data \"\"\"\n",
    "upcoming_formatted_data = upcoming_data.drop(['goals', 'rpi', 'opp_rpi'], 1)\n",
    "\n",
    "upcoming_formatted_data = upcoming_formatted_data.drop(ignore_cols + ['current_formation', 'points'], 1)\n",
    "display(upcoming_formatted_data.columns)\n",
    "display(upcoming_formatted_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# I've only implemented the linear and rbf kernels\n",
    "def kernel(params, sv, X):\n",
    "    if params[\"kernel\"] == 'linear':\n",
    "        return [np.dot(vi, X) for vi in sv]\n",
    "    elif params[\"kernel\"] == 'rbf':\n",
    "        return [math.exp(-params['gamma'] * np.vdot((vi - X).T, vi - X)) for vi in sv]\n",
    "\n",
    "# This replicates clf.decision_function(X)\n",
    "def decision_function(params, sv, nv, a, b, X):\n",
    "    # calculate the kernels\n",
    "    k = kernel(params, sv, X)\n",
    "    \n",
    "    # define the start and end index for support vectors for each class\n",
    "    start = [sum(nv[:i]) for i in range(len(nv))]\n",
    "    end = [start[i] + nv[i] for i in range(len(nv))]\n",
    "\n",
    "    # calculate: sum(a_p * k(x_p, x)) between every 2 classes\n",
    "    c = [ sum(a[ i ][p] * k[p] for p in range(start[j], end[j])) +\n",
    "          sum(a[j-1][p] * k[p] for p in range(start[i], end[i]))\n",
    "                for i in range(len(nv)) for j in range(i+1,len(nv))]\n",
    "\n",
    "    # add the intercept\n",
    "    return [sum(x) for x in zip(c, b)]\n",
    "\n",
    "# This replicates clf.predict(X)\n",
    "def predict(params, sv, nv, a, b, cs, X):\n",
    "    ''' params = model parameters\n",
    "        sv = support vectors\n",
    "        nv = # of support vectors per class\n",
    "        a  = dual coefficients\n",
    "        b  = intercepts \n",
    "        cs = list of class names\n",
    "        X  = feature to predict       \n",
    "    '''\n",
    "    decision = decision_function(params, sv, nv, a, b, X)\n",
    "    votes = [(i if decision[p] > 0 else j) for p,(i,j) in enumerate((i,j) \n",
    "                                           for i in range(len(cs))\n",
    "                                           for j in range(i+1,len(cs)))]\n",
    "\n",
    "    return cs[max(set(votes), key=votes.count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "[ 0.  0.  3.  1.  0.  0.  0.  3.  1.  0.  3.  3.  3.  3.  0.  0.  3.  3.\n",
      "  0.  0.  3.  1.  0.  1.  0.  3.  0.  0.  0.  0.  0.  1.  3.  3.  0.  3.\n",
      "  0.  1.  0.  3.  3.  0.  0.  0.  0.  3.  3.  3.  3.  3.  1.  0.  1.  0.\n",
      "  3.  3.  0.  3.  0.  0.  0.  3.  1.  3.  3.  3.  1.  3.  0.  0.  0.  3.\n",
      "  0.  1.  0.  0.  0.  0.  3.  0.  0.  3.  0.  0.  3.  3.  3.  0.  3.  0.\n",
      "  3.  0.  3.  0.  3.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Create model\n",
    "#clf = SVC(gamma=0.001, C=100.)\n",
    "\n",
    "# Fit model using features, X, and labels, Y.\n",
    "#clf.fit(X, y)\n",
    "\n",
    "svc = prediction_models[1].best_estimator_\n",
    "\n",
    "# Get parameters from model\n",
    "params = svc.get_params()\n",
    "sv = svc.support_vectors_\n",
    "nv = svc.n_support_\n",
    "a  = svc.dual_coef_\n",
    "b  = svc._intercept_\n",
    "cs = svc.classes_\n",
    "\n",
    "'''for vi in sv:\n",
    "    temp = np.array(vi - upcoming_formatted_data)\n",
    "    print(temp.shape)\n",
    "    print(type(temp))\n",
    "    np.vdot(temp, temp)\n",
    "    np.dot(temp.T, temp)np.dot(temp.T, temp)'''\n",
    "\n",
    "#print(params)\n",
    "#print(sv)\n",
    "#print(nv)\n",
    "#print(a)\n",
    "#print(b)\n",
    "#print(cs)\n",
    "\n",
    "# Use the functions to predict\n",
    "print(predict(params, sv, nv, a, b, cs, upcoming_formatted_data))\n",
    "\n",
    "# Compare with the builtin predict\n",
    "print(svc.predict(upcoming_formatted_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Models we'll use to predict on upcoming matches \"\"\"\n",
    "# pca_svc_model, knn_model, random_forest_model\n",
    "\n",
    "# This is all the X values\n",
    "upcoming_formatted_data\n",
    "\n",
    "svc_preds = prediction_models[1].predict(upcoming_formatted_data)\n",
    "svc_decsions = prediction_models[1].decision_function(upcoming_formatted_data)\n",
    "svc_probs = prediction_models[1].predict_proba(upcoming_formatted_data)\n",
    "\n",
    "print(svc_decsions.shape)\n",
    "decisions = pd.DataFrame(svc_decsions)\n",
    "display(decisions.head(1))\n",
    "probs = pd.DataFrame(svc_probs)\n",
    "probs = probs.rename(columns={0: \"A\", 1: \"B\", 2: \"C\"})\n",
    "display(probs.head(1))\n",
    "#display(decisions)\n",
    "#print(svc_decsions)\n",
    "\n",
    "rf_preds = prediction_models[2].predict(upcoming_formatted_data)\n",
    "print(rf_preds)\n",
    "knn_preds = prediction_models[0].predict(upcoming_formatted_data)\n",
    "print(knn_preds)\n",
    "\n",
    "gmm_preds = prediction_models[3].predict(upcoming_formatted_data)\n",
    "print(gmm_preds)\n",
    "\n",
    "gnb_preds = prediction_models[4].predict(upcoming_formatted_data)\n",
    "print(gnb_preds)\n",
    "\n",
    "log_preds = prediction_models[5].predict(upcoming_formatted_data)\n",
    "print(log_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['team_name', 'opp_name', 'scheduled']\n",
    "# Remove all columns except the ones above\n",
    "#upcoming_matches = upcoming_data[columns]\n",
    "upcoming_matches = upcoming_data\n",
    "\n",
    "display(probs.head(1))\n",
    "\n",
    "#random_preds = pd.Series(np.random.randint(3, size=len(upcoming_matches.index)), upcoming_matches.index)\n",
    "#random_preds[random_preds == 2] = 3\n",
    "\n",
    "# Add predictions to the end of that DF\n",
    "results = pd.DataFrame({'KNN': knn_preds, 'RandomForest': rf_preds, 'GNB': gnb_preds, 'GMM': gmm_preds, 'log': log_preds, 'SVC': svc_preds})\n",
    "\n",
    "upcoming_matches = pd.concat([upcoming_matches, results, decisions, probs], axis = 1)\n",
    "display(upcoming_matches.head(1))\n",
    "reordered_matches = pd.DataFrame([])\n",
    "\n",
    "for rows in upcoming_matches.iterrows():\n",
    "    for i in upcoming_matches['team_name']:\n",
    "        if rows[1]['opp_name'] == i:\n",
    "            reordered_matches = reordered_matches.append(rows[1])\n",
    "            reordered_matches = reordered_matches.append(upcoming_matches[upcoming_matches['team_name'].isin([i])])\n",
    "\n",
    "reordered_matches = reordered_matches.drop_duplicates() \n",
    "columns = ['scheduled', 'team_name', 'opp_name', 'goals_for', 'goals_allowed', 'opp_goals_for', 'opp_goals_allowed', \n",
    "           'goal_efficiency', 'opp_defensive_goal_efficiency', 'ratio_of_attacks', \n",
    "           'opp_ratio_of_attacks', 'ratio_ball_safe_to_dangerous_attacks', 'opp_ratio_ball_safe_to_dangerous_attacks', \n",
    "           'offensive_ranking', 'opp_defensive_ranking', 'SVC', 0, 1, 2, 'A', 'B', 'C']\n",
    "reordered_matches = reordered_matches[columns]\n",
    "reordered_matches.to_csv('predictions_on_upcoming.csv')\n",
    "print('Prediction CSV saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actual_data = pd.read_csv('predictions_on_upcoming_no_rpi_no_weights.csv')\n",
    "actual_data = actual_data.drop(actual_data.columns[[0]], axis=1)\n",
    "display(actual_data.head(1))\n",
    "actual_data['diff1'] = actual_data.apply(lambda r: model_libs.predictions_diff(r['actual'], r['SVC']), axis=1)\n",
    "accuracy = np.divide(actual_data['diff1'].sum(), float(len(actual_data['diff1'])))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "abbreviated_data = actual_data.drop([\"KNN\", \"RandomForest\", \"GNB\", \"GMM\", \"random\"], axis=1)\n",
    "print(abbreviated_data.shape)\n",
    "display(abbreviated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
