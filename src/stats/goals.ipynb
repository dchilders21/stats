{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soccer Prediction Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing needed libs and setting correct system paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZED...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import matplotlib.cm as cm\n",
    "# Show matplotlib plots inline (nicely formatted in the notebook)\n",
    "%matplotlib inline\n",
    "\n",
    "os.chdir('/Users/senzari/Machine_Learning/stats/src')\n",
    "#print(os.getcwd())\n",
    "\n",
    "from stats import form_data, match_stats, model_libs, form_model, predict_matches\n",
    "\n",
    "# Variables\n",
    "round_number = 27 # for MLS only\n",
    "ignore_cols = ['match_id', 'team_id', 'team_name', 'opp_id', 'opp_name', 'scheduled', 'games_played', 'round', 'current_formation']\n",
    "all_models = ['log', 'svc', 'knn', 'gnb', 'randomForest', 'gmm']\n",
    "leagues = model_libs.get_leagues_country_codes() # = { \"epl\": 'ENG' }\n",
    "teams = form_data.get_teams()\n",
    "league_rounds = model_libs.get_leagues_rounds()\n",
    "\n",
    "\"\"\" Change depeding on which model you want to run\"\"\"\n",
    "target = \"converted_goals\" # converted_goals or points\n",
    "\n",
    "\"\"\" this variable 'testing' should be False if using CSV's and not pulling from the database. \n",
    "    - Testers won't have database.\n",
    "    - Otherwise will build all data from scratch... takes a long time \"\"\"\n",
    "testing = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulling in the data either from the Database or the CSV (CSV for testers). Not really needed for testers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_csv = 'raw_data.csv'\n",
    "\n",
    "if testing:\n",
    "    raw_data = form_data.run_data()\n",
    "    raw_data.to_csv(data_csv)\n",
    "    print(\"Raw Data Saved to CSV\")\n",
    "else:\n",
    "    #Reading in a CSV adds the first index column\n",
    "    raw_data = pd.read_csv(data_csv)\n",
    "    raw_data = raw_data.drop(raw_data.columns[[0]], axis=1)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 85)\n",
    "print('Data Loaded...')\n",
    "print(\"Dataset size :: {}\".format(raw_data.shape))\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMATTING\n",
    "\n",
    "#### Implementing the quartiles rankings and outputs to ranked_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Helper Function - Removes Columns to Ignore and Splits the Target Column\n",
    "def split_target(data):\n",
    "    td = model_libs._clone_and_drop(data, ignore_cols)\n",
    "    (y, X) = model_libs._extract_target(td, target_col)\n",
    "    return X, y\n",
    "\n",
    "\"\"\" Need to do some formatting of the Data before we run the models\"\"\"\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "if testing:\n",
    "    ranked_data = form_data.get_rankings(leagues, teams, league_rounds, raw_data, False)\n",
    "    ranked_data.to_csv('ranked_data.csv')          \n",
    "else:\n",
    "    ranked_data = pd.read_csv('ranked_data.csv')\n",
    "    ranked_data = ranked_data.drop(ranked_data.columns[[0]], axis=1)\n",
    "\n",
    "print('Ranked Data Loaded...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING CLASSIFICATION MODEL\n",
    "\n",
    "- If using 'converted_goals' as target converts the goals to binary ( 0-1 or 2+ goals)\n",
    "- Also the area where we transform the data as desired whether using ratios or the numbers themselves\n",
    "- Then we remove the unwanted data from our DF\n",
    "- Build a simple model to confirm it's working and then check the accuracy of that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_features(data, drop_data, target, models):\n",
    "    \n",
    "    new_data = data.drop(drop_data, axis=1)\n",
    "    #display(new_data.head())\n",
    "    (y, X) = model_libs._extract_target(new_data, target)\n",
    "    models = form_model.train_models(round_number, X, y, models)\n",
    "    return models\n",
    "\n",
    "\"\"\" Formatting data to convert goals scored to the correct category\"\"\"\n",
    "formatted_data = ranked_data.copy()\n",
    "\n",
    "if target == \"converted_goals\":\n",
    "    # Not using points as a target for this version, using goals\n",
    "    formatted_data['converted_goals'] = formatted_data.apply(lambda row: model_libs.set_group(row['goals']), axis=1)\n",
    "    formatted_data = formatted_data.drop(['points', 'goals'], 1)\n",
    "else: \n",
    "    formatted_data = formatted_data.drop(['goals'], 1)\n",
    "\n",
    "    \"\"\" This is where you manipulate the features as desired \"\"\"\n",
    "\"\"\" //////////////////////////////////////////////////////////////////////////////////////////////////// \"\"\"\n",
    "\"\"\" Using diff_squared methods for features \"\"\"\n",
    "\"\"\" //////////////////////////////////////////////////////////////////////////////////////////////////// \"\"\"\n",
    "formatted_data[\"diff_goals_for\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"goals_for\"], row[\"opp_goals_for\"]), axis=1)\n",
    "formatted_data[\"diff_goals_allowed\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"goals_against\"], row[\"opp_goals_against\"]), axis=1)\n",
    "formatted_data[\"diff_attacks\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_attacks\"], row[\"opp_team_attacks\"]), axis=1)   \n",
    "formatted_data[\"diff_dangerous_attacks\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_dangerous_attacks\"], row[\"opp_team_dangerous_attacks\"]), axis=1)   \n",
    "formatted_data[\"diff_goal_attempts\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_goal_attempts\"], row[\"opp_team_goal_attempts\"]), axis=1)\n",
    "formatted_data[\"diff_ball_safe\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_ball_safe\"], row[\"opp_team_ball_safe\"]), axis=1)                                                                                                                                                           \n",
    "formatted_data[\"diff_possession\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_possession\"], row[\"opp_team_possession\"]), axis=1)\n",
    "#formatted_data[\"diff_corner_kicks\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_corner_kicks\"], row[\"opp_team_corner_kicks\"]), axis=1)                                                                                                                                                           \n",
    "#formatted_data[\"diff_goal_kicks\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_goal_kicks\"], row[\"opp_team_goal_kicks\"]), axis=1)\n",
    "#formatted_data[\"diff_saves\"] = formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_saves\"], row[\"opp_team_saves\"]), axis=1)\n",
    "\n",
    "columns_to_drop = ['current_record', 'opp_record', 'goals_for', 'opp_goals_for', 'goals_against', 'opp_goals_against', 'rpi']\n",
    "\n",
    "_, stats = form_data.get_columns()\n",
    "\n",
    "formatted_data = formatted_data.drop(ignore_cols + columns_to_drop + stats, 1)\n",
    "\n",
    "#### Running ALL Features \n",
    "if target == \"converted_goals\":\n",
    "    models_test_1 = run_features(formatted_data, [], 'converted_goals', [\"knn\"])\n",
    "    (formatted_y, formatted_X) = model_libs._extract_target(formatted_data, 'converted_goals')\n",
    "else:\n",
    "    models_test_1 = run_features(formatted_data, [], 'points', [\"knn\"])\n",
    "    (formatted_y, formatted_X) = model_libs._extract_target(formatted_data, 'points')\n",
    "    \n",
    "print(formatted_X.columns)\n",
    "\n",
    "def check_accuracy(model, data_X, y):\n",
    "    actual_y = pd.DataFrame(y.values, columns=['actual'])\n",
    "    predictions = pd.concat([pd.DataFrame(model.predict(data_X), columns=['predictions']), actual_y], axis=1)\n",
    "    predictions['accuracy'] = predictions.apply(lambda r: model_libs.predictions_diff(r['predictions'], r['actual']), axis=1)\n",
    "    accuracy = np.divide(predictions['accuracy'].sum(), float(len(predictions['accuracy'])))\n",
    "    print(accuracy)\n",
    "\n",
    "for m in models_test_1:\n",
    "    check_accuracy(m, formatted_X, formatted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run, tune, and save all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_results = []\n",
    "for m in all_models:\n",
    "    r = form_model.build_tuned_model(formatted_X, formatted_y, m)\n",
    "    model_results.append(r)\n",
    "    print('Accuracy :: ')\n",
    "    check_accuracy(r[0], formatted_X, formatted_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models have been saved so let's import them into one object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_models = form_model.load_models(['knn', 'svc', 'randomForest', 'gnb', 'log', 'gmm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the upcoming matches for the week and then ranking the quartiles like we did on the previous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Upcoming matches')\n",
    "#upcoming_matches, match_details = predict_matches.get_upcoming_matches()\n",
    "#upcoming_matches.to_csv('upcoming_matches.csv')\n",
    "upcoming_matches = pd.read_csv('upcoming_matches.csv')\n",
    "upcoming_matches = upcoming_matches.drop(upcoming_matches.columns[[0]], axis=1)\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "upcoming_data = predict_matches.predictions(upcoming_matches)\n",
    "\n",
    "if testing:\n",
    "    upcoming_ranked_data = form_data.get_rankings(leagues, teams, league_rounds, upcoming_data, True)\n",
    "    upcoming_ranked_data.to_csv('upcoming_ranked_data.csv')               \n",
    "else:\n",
    "    upcoming_ranked_data = pd.read_csv('upcoming_ranked_data.csv')\n",
    "    upcoming_ranked_data = upcoming_ranked_data.drop(upcoming_ranked_data.columns[[0]], axis=1)\n",
    "\n",
    "print('Loaded Upcoming Data...')\n",
    "#display(upcoming_ranked_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the features as need and as we did in the previous features of the build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Formatting data to convert goals scored to the correct category\"\"\"\n",
    "upcoming_formatted_data = upcoming_ranked_data.copy()\n",
    "\n",
    "\"\"\" //////////////////////////////////////////////////////////////////////////////////////////////////// \"\"\"\n",
    "\"\"\" Using diff_squared methods for features \"\"\"\n",
    "\"\"\" //////////////////////////////////////////////////////////////////////////////////////////////////// \"\"\"\n",
    "upcoming_formatted_data[\"diff_goals_for\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"goals_for\"], row[\"opp_goals_for\"]), axis=1)\n",
    "upcoming_formatted_data[\"diff_goals_allowed\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"goals_against\"], row[\"opp_goals_against\"]), axis=1)\n",
    "upcoming_formatted_data[\"diff_attacks\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_attacks\"], row[\"opp_team_attacks\"]), axis=1)   \n",
    "upcoming_formatted_data[\"diff_dangerous_attacks\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_dangerous_attacks\"], row[\"opp_team_dangerous_attacks\"]), axis=1)   \n",
    "upcoming_formatted_data[\"diff_goal_attempts\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_goal_attempts\"], row[\"opp_team_goal_attempts\"]), axis=1)\n",
    "upcoming_formatted_data[\"diff_ball_safe\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_ball_safe\"], row[\"opp_team_ball_safe\"]), axis=1)                                                                                                                                                           \n",
    "upcoming_formatted_data[\"diff_possession\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_possession\"], row[\"opp_team_possession\"]), axis=1)\n",
    "#upcoming_formatted_data[\"diff_corner_kicks\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_corner_kicks\"], row[\"opp_team_corner_kicks\"]), axis=1)                                                                                                                                                           \n",
    "#upcoming_formatted_data[\"diff_goal_kicks\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_goal_kicks\"], row[\"opp_team_goal_kicks\"]), axis=1)\n",
    "#upcoming_formatted_data[\"diff_saves\"] = upcoming_formatted_data.apply(lambda row: model_libs.diff_square(row[\"current_team_saves\"], row[\"opp_team_saves\"]), axis=1)\n",
    "\n",
    "columns_to_drop = ['current_record', 'opp_record', 'goals_for', 'opp_goals_for', 'goals_against', 'opp_goals_against', 'rpi']\n",
    "_, stats = form_data.get_columns()\n",
    "\n",
    "upcoming_formatted_data = upcoming_formatted_data.drop(ignore_cols + columns_to_drop + stats + ['points', 'goals'], 1)\n",
    "print(upcoming_formatted_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the prediction_models object to predict the upcoming matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Models we'll use to predict on upcoming matches \"\"\"\n",
    "\n",
    "# This is all the X values\n",
    "upcoming_formatted_data\n",
    "\n",
    "rf_preds = prediction_models[2].predict(upcoming_formatted_data)\n",
    "print(rf_preds)\n",
    "\n",
    "knn_preds = prediction_models[0].predict(upcoming_formatted_data)\n",
    "print(knn_preds)\n",
    "\n",
    "svc_preds = prediction_models[1].predict(upcoming_formatted_data)\n",
    "print(svc_preds)\n",
    "\n",
    "log_preds = prediction_models[3].predict(upcoming_formatted_data)\n",
    "print(log_preds)\n",
    "\n",
    "log_prob = prediction_models[3].predict_proba(upcoming_formatted_data)\n",
    "probs = pd.DataFrame(log_prob)\n",
    "#probs.to_csv('probs.csv')\n",
    "\n",
    "gnb_preds = prediction_models[3].predict(upcoming_formatted_data)\n",
    "print(gnb_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verifying the results\n",
    "\n",
    " - Takes the upcoming matches and adds the previous predictions to that DF.  \n",
    " - Also adds a random series to that DF as a baseline\n",
    " - Since the upcoming_matches DF is ordered by Team Id, we reorder it so that the Current Team row and the Opponent Team Row are next to each other for easier visualisation.\n",
    " - Adds the actual_results dataframe so we can compare the results side by side\n",
    " - Spits the results out into a CSV for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['team_name', 'opp_name', 'scheduled', 'is_home']\n",
    "# Remove all columns except the ones above\n",
    "upcoming_matches = upcoming_data[columns]\n",
    "\n",
    "if target == 'converted_goals':\n",
    "    random_preds = pd.Series(np.random.randint(2, size=len(upcoming_matches.index)), upcoming_matches.index)\n",
    "else:\n",
    "    random_preds = pd.Series(np.random.randint(3, size=len(upcoming_matches.index)), upcoming_matches.index)\n",
    "    random_preds[random_preds == 2] = 3\n",
    "\n",
    "# Add predictions to the end of that DF\n",
    "results = pd.DataFrame({'KNN': knn_preds, 'RandomForest': rf_preds, 'SVC': svc_preds, 'GNB': gnb_preds, 'log': log_preds, 'random': random_preds})\n",
    "upcoming_matches = pd.concat([upcoming_matches, results], axis = 1)\n",
    "reordered_matches = pd.DataFrame([])\n",
    "\n",
    "for rows in upcoming_matches.iterrows():\n",
    "    for i in upcoming_matches['team_name']:\n",
    "        if rows[1]['opp_name'] == i:\n",
    "            reordered_matches = reordered_matches.append(rows[1])\n",
    "            reordered_matches = reordered_matches.append(upcoming_matches[upcoming_matches['team_name'].isin([i])])\n",
    "\n",
    "reordered_matches = reordered_matches.drop_duplicates() \n",
    "columns = ['scheduled', 'team_name', 'opp_name', 'is_home', 'KNN', 'RandomForest', 'SVC', 'GNB', 'log', 'random']\n",
    "reordered_matches = reordered_matches[columns]\n",
    "actual_results = pd.read_csv('actual_results.csv')\n",
    "actual_results=actual_results.rename(columns = {'Unnamed: 0':'idx'})\n",
    "indexed_results = actual_results.set_index('idx')\n",
    "reordered_matches = pd.concat([reordered_matches, indexed_results], axis=1)\n",
    "reordered_matches = reordered_matches.reset_index(drop=True)\n",
    "display(reordered_matches.head(5))\n",
    "reordered_matches.to_csv('predictions.csv')\n",
    "print('Prediction CSV saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For converted_goals just checks to see the accuracy of the binary classifier.  For the points it gives more detail on the result.  Just need to change the column model to see which classifier you want to see the results for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_model = \"RandomForest\"  #'KNN', 'RandomForest', 'SVC', 'GNB', 'log', 'random'\n",
    "\n",
    "if target == 'converted_goals':\n",
    "    actual = \"actual_converted_goals\"\n",
    "    reordered_matches = pd.read_csv('predictions_converted_goals.csv')\n",
    "    reordered_matches = reordered_matches.drop(reordered_matches.columns[[0]], axis=1)\n",
    "    reordered_matches['accuracy'] = reordered_matches.apply(lambda r: model_libs.predictions_diff(r[actual], r[column_model]), axis=1)\n",
    "    accuracy = np.divide(reordered_matches['accuracy'].sum(), float(len(reordered_matches['accuracy'])))\n",
    "    #display(reordered_matches[[column_model, actual]])\n",
    "    print(accuracy)\n",
    "else:\n",
    "    actual = \"actual\"\n",
    "    results_data = reordered_matches.copy()\n",
    "    home_actual_win = 0\n",
    "    home_predicted_win = 0\n",
    "    predicted_draws = 0\n",
    "    actual_draws = 0\n",
    "    valid_matches = 0\n",
    "    total_matches = 0\n",
    "    invalid_predictions = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for r, rows in results_data.iterrows():\n",
    "        if r % 2 == 0:\n",
    "            total_matches += 1\n",
    "            predictions = results_data.loc[r:r+1, [\"is_home\", column_model, actual]]\n",
    "            home_team = predictions[predictions[\"is_home\"] == 1]\n",
    "            #print(predictions)\n",
    "            if home_team.iloc[0][\"actual\"] == 3:\n",
    "                home_actual_win += 1\n",
    "            elif home_team.iloc[0][\"actual\"] == 1:\n",
    "                actual_draws += 1\n",
    "\n",
    "            # Check if it's a valid prediction (0-3, 3-0, 1-1)\n",
    "            is_valid = False\n",
    "            if ((predictions.iloc[0][column_model] == 1) & (predictions.iloc[1][column_model] == 1)) or ((predictions.iloc[0][column_model] == 3) & (predictions.iloc[1][column_model] == 0)) or ((predictions.iloc[0][column_model] == 0) & (predictions.iloc[1][column_model] == 3)):\n",
    "                is_valid = True\n",
    "                valid_matches += 1\n",
    "\n",
    "                if home_team.iloc[0][column_model] == 3:\n",
    "                    home_predicted_win += 1\n",
    "\n",
    "                    if home_team.iloc[0][\"actual\"] == 3:\n",
    "                        correct_predictions += 1\n",
    "\n",
    "                if (predictions.iloc[0][column_model] == 1) & (predictions.iloc[1][column_model] == 1):\n",
    "                    predicted_draws += 1\n",
    "\n",
    "                    if home_team.iloc[0][\"actual\"] == 1:\n",
    "                        correct_predictions += 1\n",
    "            else:\n",
    "               invalid_predictions.append(predictions) \n",
    "\n",
    "    print(column_model)        \n",
    "    print('Total Matches :: {}'.format(total_matches))\n",
    "    print('Valid Predicted Matches :: {}'.format(valid_matches))\n",
    "    print('Actual Home Team Wins :: {}'.format(home_actual_win))\n",
    "    print('Home Predicted Wins :: {}'.format(home_predicted_win))\n",
    "    print('Actual Draws :: {}'.format(actual_draws))\n",
    "    print('Predicted Draws :: {}'.format(predicted_draws))\n",
    "    print('Correct Predictions :: {}'.format(correct_predictions))\n",
    "    \n",
    "    results_data['accuracy'] = results_data.apply(lambda r: model_libs.predictions_diff(r[column_model], r[actual]), axis=1)\n",
    "    accuracy = np.divide(results_data['accuracy'].sum(), float(len(results_data['accuracy'])))\n",
    "    print('Individual Accuracy :: {}'.format(accuracy))\n",
    "\n",
    "    #print(invalid_predictions)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removes half of the matches and verifies the accuracy of those predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if target == \"points\":\n",
    "    abbreviated_matches = reordered_matches.iloc[1::2, :]\n",
    "    #abbreviated_matches = reordered_matches.iloc[::2, :]\n",
    "    abbreviated_matches['accuracy'] = abbreviated_matches.apply(lambda r: model_libs.predictions_diff(r[column_model], r[\"random\"]), axis=1)\n",
    "    accuracy = np.divide(abbreviated_matches['accuracy'].sum(), float(len(abbreviated_matches['accuracy'])))\n",
    "    print('Accuracy :: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
